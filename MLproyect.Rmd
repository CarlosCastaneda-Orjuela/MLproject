---
title: "Good exercise and wearable tech Project"
author: "Carlos Castañeda-Orjuela"
output: html_document
---
# Introduction
The goal of this project is to predict the manner in which 6 people did the exercise using wearable technology.
The data for this project come from : http://groupware.les.inf.puc-rio.br/har. 

# The model selected
```{r}
setwd("~/Documents/Trabajo Epidemiología/Dr De la Hoz/Cursos/JHU_Data science Especialization/Practical Machine Learning/MLproject")
#Download the data and load in R
url1<-"https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
url2<-"https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
download.file(url1,destfile = "training.csv",method = "curl")
download.file(url2,destfile = "testing.csv",method = "curl")
training<-read.csv("training.csv",na.strings=c("","#DIV/0!"))
training<-training[,-1]
testing<-read.csv("testing.csv")
testing<-testing[,-c(1,160)]
```

I identified about 100 variables with no information in the testing database and/or in most of the trainig data observations. Those information would no be usefull to predict in the testing dataset and I ommited them from the analysis. A list of ommited variables was includen in zeroVar vector. 
```{r}
library(caret)
library(ggplot2)
zeroVar<-c("kurtosis_roll_belt","kurtosis_picth_belt","kurtosis_yaw_belt","max_roll_belt","max_picth_belt","max_yaw_belt","min_roll_belt","min_pitch_belt","min_yaw_belt","amplitude_roll_belt","amplitude_pitch_belt","amplitude_yaw_belt","var_total_accel_belt", "avg_roll_belt","stddev_roll_belt","var_roll_belt","avg_pitch_belt","stddev_pitch_belt","var_pitch_belt","avg_yaw_belt","stddev_yaw_belt","var_yaw_belt","var_accel_arm","avg_roll_arm", "stddev_roll_arm","var_roll_arm","avg_pitch_arm","stddev_pitch_arm", "var_pitch_arm","avg_yaw_arm","stddev_yaw_arm", "var_yaw_arm", "kurtosis_roll_arm", "kurtosis_picth_arm", "kurtosis_yaw_arm","skewness_roll_arm", "skewness_pitch_arm", "skewness_yaw_arm", "max_roll_arm","max_picth_arm", "max_yaw_arm","min_roll_arm","min_pitch_arm","min_yaw_arm","amplitude_roll_arm","amplitude_pitch_arm","amplitude_yaw_arm","kurtosis_roll_dumbbell","kurtosis_picth_dumbbell","kurtosis_yaw_dumbbell","skewness_roll_dumbbell","skewness_pitch_dumbbell","skewness_yaw_dumbbell", "max_roll_dumbbell","max_picth_dumbbell","max_yaw_dumbbell","min_roll_dumbbell", "min_pitch_dumbbell","min_yaw_dumbbell", "amplitude_roll_dumbbell","amplitude_pitch_dumbbell", "amplitude_yaw_dumbbell","var_accel_dumbbell", "avg_roll_dumbbell", "stddev_roll_dumbbell","var_roll_dumbbell","avg_pitch_dumbbell","stddev_pitch_dumbbell", "var_pitch_dumbbell","avg_yaw_dumbbell","stddev_yaw_dumbbell","var_yaw_dumbbell","kurtosis_roll_forearm", "kurtosis_picth_forearm","kurtosis_yaw_forearm","skewness_roll_forearm","skewness_pitch_forearm","skewness_yaw_forearm","max_roll_forearm","max_picth_forearm","max_yaw_forearm","min_roll_forearm","min_pitch_forearm","min_yaw_forearm","amplitude_roll_forearm","amplitude_pitch_forearm","amplitude_yaw_forearm","var_accel_forearm","avg_roll_forearm","stddev_roll_forearm","var_roll_forearm","avg_pitch_forearm","stddev_pitch_forearm","var_pitch_forearm","avg_yaw_forearm","stddev_yaw_forearm","var_yaw_forearm")

#Eliminate no information variables 
trainTidy<-training[-c(11:35,49:58,68:82,86:100,102:111,124:138,140:149)]
testTidy<-testing[-c(11:35,49:58,68:82,86:100,102:111,124:138,140:149)]
```

I split the training tidy database in a 70% obesrvations (train) to run the model, and 30% (test) to cross validation. Due to the amount of potentil predictors, a boosting model was implemented to identified the better model in the caret package. A boosting with trees was implemented with the method "gbm".
```{r}
#Split data in tarining for to cross validation
inTrain<-createDataPartition(y=trainTidy$classe,p = 0.7,list = FALSE)
train<-trainTidy[inTrain,]
crossVal<-trainTidy[-inTrain,]

modFit1<-train(classe~.,method = "gbm",data=train,verbose=FALSE)
#modFit2<-train(classe~.,method = "rf",data=train)
```

# Expected error out of the sample
I evaluate the cross validation with the test part in the trainin database. I estimated the accuracy of the model in 99.6% in teh test part of the training data set. The expected error in out of sample prediction is <0.4%.
```{r}
predCross<-predict(modFit1,crossVal)
table(predCross,crossVal$classe)
mean(predCross==crossVal$classe)
```

# Prediction in the 20 samples
The predicted classes in the 20 samples are:
```{r}
answers<-predict(modFit1,testTidy)
answers
pml_write_files = function(x){
  n = length(x)
  for(i in 1:n){
    filename = paste0("problem_id_",i,".txt")
    write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
  }
}
pml_write_files(answers)
#predict(modFit2,testing)
```
